{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 05) Temporal-Difference Learning\n",
    "\n",
    "In this exercise we again use the racetrack environment developed in the first homework.\n",
    "You can either use our provided environment or your own solution from the homework assignment.  \n",
    "\n",
    "For starting, please execute the following cells. \n",
    "There, we will build the more complex rectangular course which was used in the last task of exercise 04.\n",
    "\n",
    "A dummy policy is defined, which turns the car to the right in front of a wall.\n",
    "As a reminder the action encoding can be seen in the following picture:\n",
    "\n",
    "![](Directions_Legend.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from solution import RaceTrackEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "#plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the course\n",
    "_course_dim = (8, 10)\n",
    "_inner_wall_dim = (2, 6)\n",
    "\n",
    "def build_rect_course(course_dim, inner_wall_dim):\n",
    "    \"\"\"Build a race track given specifications for the outer cyclic street and inner wall dimensions.\n",
    "    Start and finish line should be placed in the center top. The course dimension specifications\n",
    "    do not consider a bounding wall around the track, which must be inserted additionally.\n",
    "    \n",
    "    Args:\n",
    "        course_dim: 2-tuple, (y-dim, x-dim): The size of the track without outer walls.\n",
    "        inner_wall_dim: 2-tuple (y-dim, x-dim): The size of the inner wall\n",
    "    \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    track = []\n",
    "    wall_up_bound = course_dim[0]//2 - inner_wall_dim[0] // 2\n",
    "    wall_bottom_bound = course_dim[0]//2 + inner_wall_dim[0]//2\n",
    "    street_width = course_dim[1]//2 - inner_wall_dim[1]//2\n",
    "    # construct course line by line\n",
    "    for i in range(course_dim[0]):\n",
    "        if i < wall_up_bound:\n",
    "            half_street_len = course_dim[1]//2 - 1\n",
    "            track_row = 'o'*half_street_len + '+W-' + 'o'*(half_street_len-1)\n",
    "        elif  wall_up_bound <= i < wall_bottom_bound:\n",
    "            track_row = 'o'*street_width + 'W'*inner_wall_dim[1] + 'o'*street_width\n",
    "        else:\n",
    "            track_row = 'o'*course_dim[1]\n",
    "        track.append(track_row)\n",
    "    # add boundary\n",
    "    track = ['W'*course_dim[1]] + track + ['W'*course_dim[1]]\n",
    "    track = ['W'+s+'W' for s in track]\n",
    "    return track\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course = build_rect_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "dummy_slow_pi = np.ones([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY]) * 4 \n",
    "\n",
    "dummy_slow_pi[:track.bounds[0]//2, :, 0 , 0] = 5 # turn right\n",
    "dummy_slow_pi[:track.bounds[0]//2, -2:, 0 , :] = 6 # turn down left\n",
    "dummy_slow_pi[-2:, track.bounds[1]//2:, : , 0] = 0 # turn up left\n",
    "dummy_slow_pi[track.bounds[0]//2:, :2, 0, :] = 2 # turn up right\n",
    "dummy_slow_pi[:2, 0:track.bounds[1]//2, :, 0] = 8 # turn down right\n",
    "\n",
    "\n",
    "pi = dummy_slow_pi\n",
    "\n",
    "y_size, x_size = track.bounds\n",
    "\n",
    "# Run learned policy on test case\n",
    "pos_map = np.zeros((y_size, x_size))\n",
    "p, v = track.reset()\n",
    "for k in range(2000):\n",
    "    s_y, s_x = p[0], p[1]\n",
    "    s_vy, s_vx = v[0], v[1]\n",
    "    pos_map[s_y, s_x] += 1  # exploration map\n",
    "    act = track.action_to_tuple(pi[s_y, s_x, s_vy, s_vx])\n",
    "    (p, v), rew, done, _ = track.step(act)\n",
    "    if done: break    \n",
    "\n",
    "for row in course:\n",
    "    print(row)\n",
    "\n",
    "print('\\n \\n Sample trajectory on dummy policy:')\n",
    "pos_map = (pos_map > 0).astype(np.float32)\n",
    "pos_map +=  track.course  # overlay track course\n",
    "plt.imshow(pos_map, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) TD-Based Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a TD-based prediction algorithm to evaluate the dummy policy using $\\alpha = 0.2$ and $\\gamma = 1$ and calculate the state values.\n",
    "\n",
    "After how many episodes do the state values converge?\n",
    "Compare this to Monte-Carlo first visit prediciton from exercise 04.\n",
    "\n",
    "Change $\\alpha$ to $1$? Does it work? Explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to print the state values\n",
    "def text_print_pos_map(_pos_map):\n",
    "    for row in _pos_map:\n",
    "        print(' '.join(x_size*['{}']).format(*[str(int(r)).zfill(3) for r in row]))\n",
    "\n",
    "# Function to plot the heatmap \n",
    "def plot_pos_map(_pos_map):\n",
    "    plt.imshow(_pos_map, cmap='hot', interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "# Initialise state values \n",
    "values = ???\n",
    "\n",
    "# Configuration parameters\n",
    "gamma = 1\n",
    "alpha = 0.2\n",
    "\n",
    "# Initialise race track course\n",
    "course = course\n",
    "track = RaceTrackEnv(course)\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "\n",
    "pos_map = np.zeros((y_size, x_size))\n",
    "\n",
    "\n",
    "episodes = 250\n",
    "\n",
    "for e in tqdm(range(episodes), desc='episode'): \n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    # initialize x0\n",
    "    p, v = track.reset()\n",
    "    \n",
    "    # x0\n",
    "    s_y, s_x = p[0], p[1]\n",
    "    s_vy, s_vx = v[0], v[1]\n",
    "    state_tuple =  s_y, s_x, s_vy, s_vx\n",
    "  \n",
    "    # episodes do not terminate by time limit\n",
    "    while not done:\n",
    "        \n",
    "        ???      \n",
    "        \n",
    "            \n",
    "    for s_x in range(x_size):\n",
    "        for s_y in range(y_size):\n",
    "            pos_map[s_y, s_x] = np.min(values[s_y, s_x, :, :])\n",
    "     \n",
    "    # Print state values after each episode\n",
    "    print('\\n \\n Estimated state values in episode {}: \\n'.format(e))\n",
    "    text_print_pos_map(pos_map)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Plot heatmap in the end\n",
    "plot_pos_map(-pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) On-Policy $\\varepsilon$-Greedy Control: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a temporal-difference based $\\varepsilon$-greedy control algorithm to solve the rectangular course environment used above. \n",
    "Use $\\varepsilon = 0.1$, $\\alpha = 0.5$ and $\\gamma = 1$ and run $500$ episodes.\n",
    "\n",
    "Note that no initial policy is needed for TD control methods. \n",
    "\n",
    "\n",
    "Does Sarsa perform good at learning an optimal greedy policy?\n",
    "\n",
    "Change $\\alpha$ to $0.1$ and $0.9$. What do you recognize? Explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialise action values \n",
    "action_values = ???\n",
    "\n",
    "cumulated_rewards = []\n",
    "\n",
    "# configuration parameters\n",
    "epsilon = 0.1   # exploration probability\n",
    "gamma = 1       # discount factor\n",
    "alpha = 0.5     # forgetting factor\n",
    "episodes = 500 # number of evaluated episodes\n",
    "\n",
    "# define track\n",
    "course = course\n",
    "track = RaceTrackEnv(course)\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "\n",
    "\n",
    "\n",
    "for e in tqdm(range(episodes), desc='episode'): \n",
    "    \n",
    "    ???\n",
    "    \n",
    "    # x_k = x0\n",
    "    ???\n",
    "    \n",
    "    # Choose Initial Action greedy\n",
    "    ???\n",
    "        \n",
    "    # episodes do not terminate by time limit\n",
    "    while not done:\n",
    "        \n",
    "        \n",
    "        # save state_action tuple before step\n",
    "        ???\n",
    "        \n",
    "        # take environment step\n",
    "        (p, v), reward, done, _ = ???\n",
    "        \n",
    "        cumulated_reward += reward\n",
    "        \n",
    "        # x_k+1\n",
    "        ???\n",
    "        \n",
    "        ???\n",
    "        \n",
    "            \n",
    "    cumulated_rewards.append(cumulated_reward)      \n",
    "        \n",
    "    \n",
    "plt.plot(cumulated_rewards)\n",
    "plt.title('Training Progress using alpha = {}:'.format(alpha))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulated Reward')\n",
    "plt.ylim(-2000, 0) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to apply the best policy. Due to the random start position, do $5$ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, n_episodes=5):\n",
    "    for e in range(n_episodes):\n",
    "        action_states = []\n",
    "        rewards = []\n",
    "\n",
    "        pos_map = np.zeros((y_size, x_size))\n",
    "        p, v = track.reset()\n",
    "        for k in range(200):\n",
    "            s_y, s_x = p[0], p[1]\n",
    "            s_vy, s_vx = v[0], v[1]\n",
    "\n",
    "            pos_map[s_y, s_x] += 1  # exploration map\n",
    "\n",
    "            state_tuple = s_y, s_x, s_vy, s_vx\n",
    "            action = np.argmax(policy[state_tuple])\n",
    "\n",
    "            a = track.action_to_tuple(action)\n",
    "            action_state = track.state_action((p, v), a)\n",
    "            action_states.append(action_state)\n",
    "\n",
    "            (p, v), reward, done, _ = track.step(a)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                print('Done')\n",
    "                break \n",
    "\n",
    "\n",
    "        print('Sample trajectory on learned policy in episode {}:'.format(e))\n",
    "        pos_map = (pos_map > 0).astype(np.int16)\n",
    "        pos_map +=  track.course  # overlay track course\n",
    "        plt.imshow(pos_map, cmap='hot', interpolation='nearest')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "evaluate_policy(action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Off-Policy $\\varepsilon$-Greedy Control: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function for a Q-Learning algorithm to solve the rectangular course environment (function will be re-used in the next task).\n",
    "Use again $\\varepsilon = 0.1$, $\\alpha = 0.5$, $\\gamma = 1$ and 500 episodes.\n",
    "\n",
    "Can the resulting greedy policy be expected to be better or worse than the optimal policy trained with Sarsa?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def q_learning(epsilon, gamma, alpha, episodes, track):\n",
    "    \"\"\"\n",
    "    Defines the Q-learning function which performs ùúÄ-greedy control on the given track.\n",
    "\n",
    "    :epsilon:  exploration probability\n",
    "    :gamma:    discount factor\n",
    "    :alpha:    forgetting factor\n",
    "    :episodes: number of evaluated episodes\n",
    "    :tack:     race track to learn\n",
    "    \n",
    "    :return: list of cummulated rewards per episode and action values\n",
    "    \"\"\"\n",
    "    # Initialise action values \n",
    "    action_values = ???\n",
    "    \n",
    "    cumulated_rewards = []\n",
    "\n",
    "\n",
    "    for e in tqdm(range(episodes), desc='episode'): \n",
    "        \n",
    "        ???\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            ???\n",
    "            \n",
    "            (p, v), reward, done, _ = track.step(???)\n",
    "\n",
    "            # Sum up reward\n",
    "            cumulated_reward += reward\n",
    "            \n",
    "            ???\n",
    "\n",
    "        #print(cumulated_reward)\n",
    "        cumulated_rewards.append(cumulated_reward) \n",
    "        \n",
    "    return cumulated_rewards, action_values\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# configuration parameters\n",
    "epsilon = 0.1   # exploration probability\n",
    "gamma = 1       # discount factor\n",
    "alpha = 0.5      # forgetting factor\n",
    "episodes = 500 # number of evaluated episodes\n",
    "\n",
    "# define track using the course defined above\n",
    "course = course\n",
    "track = RaceTrackEnv(course)\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "\n",
    "cumulated_rewards, action_values = q_learning(epsilon, gamma, alpha, episodes, track)\n",
    "\n",
    "plt.plot(cumulated_rewards)\n",
    "plt.title('Training Progress using alpha = {}:'.format(alpha))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulated Reward')\n",
    "plt.ylim(-2000, 0) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show again the best 5 episodes using the function defined in 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_policy(action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Double Q-Learning\n",
    "\n",
    "Now assume the driver had a few beers. Due to the alcohol he/she is not able to perceive the environment in detail.\n",
    "The reward is therefore stochastic whenever an action, that accelerates the car into the direction of a corner, is selected (e.g. [-1,1] in the upper right quadrant).\n",
    "Therefore a stochastic environment is given below. \n",
    "Execute the cell before continuing with the exercise.\n",
    "\n",
    "- Use this environment and try to solve it with the Q-learning function written in 3) using $\\varepsilon = 0.1$, $\\alpha = 0.5$ and $\\gamma = 1$ with the cells given below.\n",
    "\n",
    "A code template is given which repeats the training $3$ times to calculate the mean cumulated reward.\n",
    "\n",
    "\n",
    "- Write a double Q-learning algorithm to solve the above defined course with the given template.\n",
    "Use again $\\varepsilon = 0.1$, $\\alpha = 0.5$ and $\\gamma = 1$.\n",
    "\n",
    "Compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drunken Driver Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochRaceTrackEnv(RaceTrackEnv):\n",
    "    \"\"\"Driver is not able to perceive its environment clearly anymore\"\"\"\n",
    "    def __init__(self, eps=0.2, **kwargs):\n",
    "        self.eps = eps        \n",
    "        super().__init__(**kwargs)\n",
    "        # centrifugal acceleration map\n",
    "        self.cam = np.zeros((2,) + self.course.shape, dtype=int)\n",
    "        self.cam[0, :self.course.shape[0]//2, :] = -1\n",
    "        self.cam[0, self.course.shape[0]//2:, :] = 1\n",
    "        self.cam[1, :, :self.course.shape[1]//2] = -1\n",
    "        self.cam[1, :, self.course.shape[1]//2:] = 1\n",
    "        \n",
    "    def step(self, action):\n",
    "        if isinstance(action, int):\n",
    "            action = self.action_to_tuple(action)\n",
    "        \n",
    "        prev_p, prev_v = tuple(self.position), tuple(self.velocity)\n",
    "        \n",
    "        \n",
    "        (p, v), reward, done, _ = super().step(action)\n",
    "        \n",
    "        # Abandoning oneself to physical drag might seem relieving\n",
    "        #  but is deadly\n",
    "        if action[0] == self.cam[0, prev_p[0], prev_p[1]] and action[1] == self.cam[1, prev_p[0], prev_p[1]]:\n",
    "            reward += np.random.randn()\n",
    "\n",
    "        return (p, v), reward, done, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "epsilon = 0.1   # exploration probability\n",
    "gamma = 1       # discount factor\n",
    "alpha = 0.5     # forgetting factor\n",
    "episodes = 400  # number of evaluated episodes\n",
    "repetitions = 3 # repeats the training to get mean reward\n",
    "\n",
    "# define track using the course defined above\n",
    "track = StochRaceTrackEnv(course=course)\n",
    "\n",
    "reward_trajectories_Q = []\n",
    "policies_Q = []\n",
    "\n",
    "# Runs training repetitions-times\n",
    "for _ in tqdm(range(repetitions), desc='Experiment'):\n",
    "    # Execute q-learning\n",
    "    cumulated_rewards, action_values = q_learning(epsilon, gamma, alpha, episodes, track)\n",
    "    \n",
    "    # store reward and policy\n",
    "    reward_trajectories_Q.append(cumulated_rewards)\n",
    "    policies_Q.append(action_values)\n",
    "    \n",
    "plt.plot(np.vstack(reward_trajectories_Q).mean(axis=0))\n",
    "plt.title('Q-learning')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean Cumulated Reward')\n",
    "plt.ylim(-400, 0) \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show episodes\n",
    "# Uses last calulated Q-values to evaluate the policy\n",
    "evaluate_policy(policies_Q[-1], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def double_q_learning(_epsilon, _gamma, _alpha, _episodes, _track):\n",
    "    \"\"\"\n",
    "    Defines the Q-learning function which performs ùúÄ-greedy control on the given track.\n",
    "\n",
    "    :_epsilon:  exploration probability\n",
    "    :_gamma:    discount factor\n",
    "    :_alpha:    forgetting factor\n",
    "    :_episodes: number of evaluated episodes\n",
    "    :_tack:     race track to learn\n",
    "    \n",
    "    :return: list of cummulated rewards per episode and action values\n",
    "    \"\"\"\n",
    "    \n",
    "    course = _track.course\n",
    "    x_size, y_size = len(course[0]), len(course)\n",
    "    action_values1 = ???\n",
    "    action_values2 = ???\n",
    "    cumulated_rewards = []\n",
    "    \n",
    "    for e in tqdm(range(_episodes), desc='episode'): \n",
    "        \n",
    "        cumulated_reward = 0\n",
    "        \n",
    "        ???\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            ???\n",
    "            \n",
    "            cumulated_reward += reward\n",
    "\n",
    "        cumulated_rewards.append(cumulated_reward)      \n",
    "    return cumulated_rewards, action_values1+action_values2\n",
    "\n",
    "\n",
    "reward_trajectories_double_Q = []\n",
    "policies_double_Q = []\n",
    "\n",
    "# Runs training repetitions-times\n",
    "for _ in tqdm(range(repetitions), desc='Experiment'):\n",
    "    \n",
    "    # Execute double q-learning\n",
    "    cumulated_rewards, action_values = double_q_learning(epsilon, gamma, alpha, episodes, track)\n",
    "    \n",
    "    # store reward and policy\n",
    "    reward_trajectories_double_Q.append(cumulated_rewards)\n",
    "    policies_double_Q.append(action_values)\n",
    "    \n",
    "plt.plot(np.vstack(reward_trajectories_Q).mean(axis=0), label='Q Learning')\n",
    "plt.plot(np.vstack(reward_trajectories_double_Q).mean(axis=0), label='Double Q')\n",
    "plt.title('Q Learning vs Double Q Learning')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean Cumulated Reward')\n",
    "plt.ylim(-400, 0) \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.vstack(reward_trajectories_Q).mean(axis=0), label='Q Learning')\n",
    "plt.plot(np.vstack(reward_trajectories_double_Q).mean(axis=0), label='Double Q')\n",
    "plt.title('Q Learning vs Double Q Learning')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Mean Cumulated Reward')\n",
    "plt.ylim(-400, 0) \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show again the best $5$ episodes using the function defined in 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uses last calulated Q-values to evaluate the policy\n",
    "evaluate_policy(policies_double_Q[-1], 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
