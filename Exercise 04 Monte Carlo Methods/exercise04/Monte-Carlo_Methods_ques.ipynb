{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04) Monte-Carlo Methods\n",
    "\n",
    "In this exercise we want to make use of the racetrack environment developed in the first homework. \n",
    "If you were not able to finish this homework, an adequate environment is provided within this folder.\n",
    "Feel free to test your version of the environment and compare the results.\n",
    "Also, be explorative and alternate some configuration parameters to see if they have an impact upon the result.\n",
    "\n",
    "For the start, please execute the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from solution import RaceTrackEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWWWWWWWWWWW\n",
      "WWWWW-oooooW\n",
      "WWWWW-oooooW\n",
      "WWWWW-oooooW\n",
      "WWWWWWWWWooW\n",
      "WWWWWWWWWooW\n",
      "WWWWW+oooooW\n",
      "WWWWW+oooooW\n",
      "WWWWW+oooooW\n",
      "WWWWWWWWWWWW\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the course\n",
    "_course_dim = (8, 10)\n",
    "_inner_wall_dim = (2, 6)\n",
    "\n",
    "def build_uturn_course(course_dim, inner_wall_dim):\n",
    "    \"\"\"\n",
    "    Build a race track for the u-turn street scenario.\n",
    "    Start and finish line are placed in the center top and bottom respectively. The course dimension specifications\n",
    "    do not consider a bounding wall around the track, which is inserted additionally. \n",
    "\n",
    "    \"\"\"\n",
    "    track = []\n",
    "    wall_up_bound = course_dim[0]//2 - inner_wall_dim[0] // 2\n",
    "    wall_bottom_bound = course_dim[0]//2 + inner_wall_dim[0]//2\n",
    "    street_width = course_dim[1]//2 - inner_wall_dim[1]//2\n",
    "    # construct course line by line\n",
    "    for i in range(course_dim[0]):\n",
    "        if i < wall_up_bound:\n",
    "            half_street_len = course_dim[1]//2 - 1\n",
    "            track_row = 'W'*(half_street_len//2+1) + 'W-' + 'o'*(half_street_len-1+half_street_len//2)\n",
    "        elif  wall_up_bound <= i < wall_bottom_bound:\n",
    "            track_row = 'W'*street_width + 'W'*inner_wall_dim[1] + 'o'*street_width\n",
    "        else:\n",
    "            track_row = 'W'*(half_street_len//2+1) + 'W+' + 'o'*(half_street_len-1+half_street_len//2)\n",
    "        track.append(track_row)\n",
    "    # add boundary\n",
    "    track = ['W'*course_dim[1]] + track + ['W'*course_dim[1]]\n",
    "    track = ['W'+s+'W' for s in track]\n",
    "    return track\n",
    "    \n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "for row in course:\n",
    "    print(row)\n",
    "    \n",
    "pos_map =  track.course  # overlay track course\n",
    "plt.imshow(pos_map, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) MC-Based Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a first-visit Monte-Carlo algorithm to evaluate the dummy policy as defined below on the U-turn course. The dummy policy turns the car to the right as soon as it stands in front of a wall. Try to understand how the policy works before you start to code. Actions (accelerations in given directions) are encoded according to the following diagram:\n",
    "\n",
    "![](Directions_Legend.png)\n",
    "\n",
    "How can we interprete the state values resulting from the evaluation with first-visit Monte-Carlo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select course and initialize dummy policy\n",
    "\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "dummy_slow_pi = np.ones([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY]) * 4 \n",
    "\n",
    "dummy_slow_pi[:track.bounds[0]//2, :, 0 , 0] = 5   # go right\n",
    "dummy_slow_pi[:track.bounds[0]//2, -2:, 0 , :] = 6 # go bottom left\n",
    "dummy_slow_pi[-2:, track.bounds[1]//2:, : , 0] = 0 # go top left\n",
    "\n",
    "pi = dummy_slow_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the value function\n",
    "values = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY])\n",
    "\n",
    "# initialize an empty dict to count the number of visits\n",
    "n_dict = {}\n",
    "\n",
    "# configuration parameters\n",
    "gamma = 1 # discount factor\n",
    "no_episodes = 500 # number of evaluated episodes\n",
    "no_steps = 2000 # number of allowed timesteps per episode\n",
    "\n",
    "\n",
    "for e in tqdm(range(no_episodes), position=0, leave=True):\n",
    "    \n",
    "    # initialize variables in which collected data will be stored\n",
    "    states = ??? # list of tuples\n",
    "    rewards = ??? # list of floats\n",
    "    visited_states = ??? # set of tuples\n",
    "    first_visit_list = ??? # list of booleans\n",
    "    \n",
    "    # reset environment and start episode\n",
    "    p, v = track.reset()\n",
    "    for k in range(no_steps):\n",
    "        \n",
    "        # unpack the statee information\n",
    "        s_y, s_x = p[0], p[1]\n",
    "        s_vy, s_vx = v[0], v[1]\n",
    "        state_tuple = s_y, s_x, s_vy, s_vx\n",
    "        \n",
    "        # save the momentary state\n",
    "        states.append(???) \n",
    "        \n",
    "        # check momentary state for first visit\n",
    "        first_visit_list.append(???)\n",
    "        visited_states.add(???)\n",
    "        \n",
    "        # choose and perform action\n",
    "        action = track.action_to_tuple(???)\n",
    "        (p, v), reward, done, _ = track.step(action)\n",
    "        \n",
    "        # save received reward\n",
    "        rewards.append(???)\n",
    "        \n",
    "        # terminate the environment if the finish line was passed\n",
    "        if done: \n",
    "            break \n",
    "             \n",
    "    # learn from the collected data\n",
    "    g = 0  \n",
    "    for s, r, first_visit in zip(states[???], rewards[???], first_visit_list[???]): # count backwards\n",
    "        g = ???\n",
    "        \n",
    "        if first_visit:\n",
    "            \n",
    "            # Count visits to this state in n_list\n",
    "            n_dict[s] = n_dict.get(s, 0) + ???\n",
    "\n",
    "            # add new return g to existing value\n",
    "            values[s] += ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the result of the evaluation, plot the state values as a function of **position only** (so that you get a two dimensional representation of the state value) and in the form of a tabular represenation and a heatmap. In order to omit dependence of the velocity dimensions, use the minimum of the value function with respect to the velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def text_print_pos_map(_pos_map):\n",
    "    for row in _pos_map:\n",
    "        print(' '.join(x_size*['{}']).format(*[str(int(r)).zfill(3) for r in row]))\n",
    "        \n",
    "def plot_pos_map(_pos_map):\n",
    "    plt.imshow(???) # use the \"hot\" colormap and the \"nearest\" interpolation\n",
    "    plt.show()\n",
    "\n",
    "# calculate minimum value with respect to velocities\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "pos_map = np.zeros((y_size, x_size))\n",
    "\n",
    "for s_x in range(x_size):\n",
    "    for s_y in range(y_size):\n",
    "        pos_map[s_y, s_x] = ???\n",
    "        \n",
    "text_print_pos_map(pos_map)\n",
    "plot_pos_map(-pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) On-Policy $\\varepsilon$-Greedy Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the previously used turn-right-if-wall dummy policy, write an on-policy Monte-Carlo based first-visit $\\varepsilon$-greedy control algorithm to solve the U-turn course. The policy is now stochastic: it does not contain simple action commands for each state, but probabilities for each possible action. Again, please make sure to understand how the stochastic policy works before coding.\n",
    "\n",
    "\n",
    "Make sure to implement an upper bound for episode length (we suggest a boundary of 200 steps). Why do we need a bound like this? What happens to the state values / state-action values if we increase the bound?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dummy policy\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "\n",
    "dummy_slow_stoch_pi = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 9])\n",
    "\n",
    "dummy_slow_stoch_pi[  :,   :, :, :, 4] = 1 # set probability of doing nothing to one for every state\n",
    "\n",
    "# set probability to go right:\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 5] = 1 \n",
    "# set probability to do nothing where we want to go right:\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 4] = 0 \n",
    "\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 6] = 1 # probability to go bottom left\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 4] = 0 \n",
    "\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 0] = 1 # probability to go top left\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 4] = 0 \n",
    "\n",
    "pi = dummy_slow_stoch_pi                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize action_values and counting dict\n",
    "action_values = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 3, 3])\n",
    "n_dict = {}\n",
    "\n",
    "# configuration parameters\n",
    "epsilon = 0.1 # exploration probability\n",
    "gamma = 1 # discount factor\n",
    "no_episodes = 5000 # number of evaluated episodes\n",
    "no_steps = 200 # number of evaluated timesteps per episode\n",
    "\n",
    "\n",
    "track = RaceTrackEnv(course)\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "\n",
    "for e in tqdm(range(no_episodes), desc='episode', mininterval=2):\n",
    "      \n",
    "    # initialize variables in which collected data will be stored\n",
    "    action_states = ??? # list of tuples\n",
    "    rewards = ??? # list of floats\n",
    "    visited_action_states = set() # set of tuples\n",
    "    first_visit_list = ??? # list of booleans\n",
    "    \n",
    "    pos_map = np.zeros((y_size, x_size)) # initializes a map that can be plotted\n",
    "    p, v = track.reset()\n",
    "    for k in range(no_steps):\n",
    "        s_y, s_x = p[0], p[1]\n",
    "        s_vy, s_vx = v[0], v[1]\n",
    "        \n",
    "        pos_map[s_y, s_x] += 1  # mark the visited position on the map\n",
    "        \n",
    "        # execute action (either by following the policy, or by exploring randomly)\n",
    "        if epsilon < np.random.rand(1):\n",
    "            action = ???\n",
    "        else:\n",
    "            action = ???\n",
    "        \n",
    "        # save the action state and check for first visit\n",
    "        a = track.action_to_tuple(action)\n",
    "        action_state = track.state_action((p, v), a)\n",
    "        action_states.append(???)\n",
    "        ???\n",
    "        visited_action_states.add(action_state)\n",
    "        \n",
    "        # perform action\n",
    "        (p, v), reward, done, _ = ???\n",
    "        \n",
    "        # save received reward\n",
    "        ???\n",
    "        \n",
    "        # terminate the environment if the finish line was passed\n",
    "        ???\n",
    "    \n",
    "    # learn from the collected data\n",
    "    g = 0   \n",
    "    for r, a_s, first_visit in zip(rewards[???], action_states[???], first_visit_list[???]): # count backwards\n",
    "        g = ???\n",
    "        \n",
    "        if first_visit:\n",
    "            \n",
    "            # Count visits to this state in n_list\n",
    "            n_dict[a_s] = ???\n",
    "\n",
    "            # add new return g to existing value\n",
    "            action_values[a_s] += ???\n",
    "                        \n",
    "            # calculate the new action probabilities\n",
    "            u_best = np.argmax(???)\n",
    "            pi[a_s[:4]] = epsilon / 9\n",
    "            pi[a_s[:4]][u_best] = ???\n",
    "            \n",
    "    \n",
    "    # this code fragment is to plot the sampled map, comment out for faster computation\n",
    "    print('Sample trajectory on learned policy in episode {}:'.format(e))\n",
    "    pos_map = (pos_map > 0).astype(np.float32)\n",
    "    pos_map +=  track.course  # overlay track course\n",
    "    plot_pos_map(pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code block directly below to test the resulting deterministic greedy policy (several samples are taken in order to show behavior in all different starting positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_episodes = 10\n",
    "for e in range(no_episodes):\n",
    "    \n",
    "    pos_map = np.zeros((y_size, x_size))\n",
    "    p, v = track.reset()\n",
    "    for k in range(200):\n",
    "        s_y, s_x = p[0], p[1]\n",
    "        s_vy, s_vx = v[0], v[1]\n",
    "        \n",
    "        pos_map[s_y, s_x] += 1  # exploration map\n",
    "        \n",
    "        action = ???\n",
    "        a = track.action_to_tuple(action)\n",
    "        action_state = track.state_action((p, v), a)\n",
    "\n",
    "        (p, v), reward, done, _ = ???\n",
    "\n",
    "        if done:\n",
    "            break \n",
    "\n",
    "    print('Sample trajectory on learned policy in episode {}:'.format(e))\n",
    "    pos_map = (pos_map > 0).astype(np.int16)\n",
    "    pos_map +=  track.course  # overlay track course\n",
    "    plot_pos_map(pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Off-Policy $\\varepsilon$-Greedy Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dummy-policy from (2) as a behavior policy, write an off-policy Monte-Carlo algorithm with weighted importance sampling.\n",
    "\n",
    "Has the result gotten better or worse? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dummy Policy\n",
    "course = build_uturn_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "dummy_slow_stoch_pi = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 9])\n",
    "\n",
    "# as the behavior policy is not alternated, there is no possibility to implement the epsilon parameter later\n",
    "# hence, we need to implemented it right here\n",
    "epsilon = 0.1\n",
    "\n",
    "dummy_slow_stoch_pi[  :,   :, :, :, 4] = 1 - epsilon + epsilon / 9\n",
    "for i in range(9):\n",
    "    if i != 4:\n",
    "        dummy_slow_stoch_pi[  :, :, :, :, i] = epsilon / 9\n",
    "    \n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 5] = 1-epsilon + epsilon/9\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, :, 0 , 0, 4] = epsilon / 9\n",
    "\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 6] = 1-epsilon + epsilon/9\n",
    "dummy_slow_stoch_pi[:track.bounds[0]//2, -2:, 0 , :, 4] = epsilon / 9\n",
    "\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 0] = 1-epsilon + epsilon/9\n",
    "dummy_slow_stoch_pi[-2:, track.bounds[1]//2:, : , 0, 4] = epsilon / 9\n",
    "\n",
    "behavior_policy = dummy_slow_stoch_pi  \n",
    "\n",
    "pi = np.copy(behavior_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize action_values and dict of cumulated WIS weights\n",
    "action_values = np.zeros([track.bounds[0], track.bounds[1], 1+2*track.MAX_VELOCITY, 1+2*track.MAX_VELOCITY, 3, 3])\n",
    "c_dict = {}\n",
    "\n",
    "# configuration parameters\n",
    "# epsilon = 0.1 was defined within the behavior policy\n",
    "gamma = 1 # discount factor\n",
    "no_episodes = 1000 # number of evaluated episodes\n",
    "no_steps = 200 # number of evaluated timesteps per episode\n",
    "\n",
    "course = course\n",
    "track = RaceTrackEnv(course)\n",
    "x_size, y_size = len(course[0]), len(course)\n",
    "\n",
    "for e in tqdm(range(no_episodes), desc='episode', mininterval=2):\n",
    "    \n",
    "    action_states = ??? # list of tuples\n",
    "    actions = ??? # list of tuples\n",
    "    rewards = ??? # list of floats\n",
    "    \n",
    "    pos_map = np.zeros((y_size, x_size))\n",
    "    p, v = track.reset()\n",
    "    for k in range(no_steps):\n",
    "        s_y, s_x = p[0], p[1]\n",
    "        s_vy, s_vx = v[0], v[1]\n",
    "        \n",
    "        pos_map[s_y, s_x] += 1  # exploration map\n",
    "        \n",
    "        if epsilon < np.random.rand(1):\n",
    "            action = np.argmax(???])\n",
    "        else:\n",
    "            action = ???\n",
    "        \n",
    "        a = track.action_to_tuple(action)\n",
    "        action_state = track.state_action((p, v), a) # saves the action_state to be used as an index (all values are positive)\n",
    "        actions.append(???) # saves the action as it is applied (acceleration can be negative)\n",
    "        action_states.append(action_state)\n",
    "\n",
    "        (p, v), reward, done, _ = ???\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            ???\n",
    "    \n",
    "    # Monte-Carlo WIS Update\n",
    "    g = ???\n",
    "    w = ???       \n",
    "    for r, a_s, applied_action in zip(rewards[???], action_states[???], actions[???]): # count backwards\n",
    "        g = ???\n",
    "            \n",
    "        # Count visits to this state in n_list\n",
    "        c_dict[a_s] = ???\n",
    "\n",
    "        # add new return g to existing value\n",
    "        action_values[a_s] += ???\n",
    "                        \n",
    "        # determine greedy policy\n",
    "        u_best = np.argmax(???)\n",
    "        pi[a_s[:4]] = 0\n",
    "        pi[a_s[:4]][u_best] = 1\n",
    "        \n",
    "        # check if applied action equals greedy action\n",
    "        if ??? != track.action_to_tuple(???):\n",
    "            break\n",
    "        \n",
    "        w = ???\n",
    "    \n",
    "    # code fragment for plotting      \n",
    "    pos_map = (pos_map > 0).astype(np.float32)\n",
    "    pos_map +=  track.course  # overlay track course\n",
    "    print('Sample trajectory on learned policy in episode {}:'.format(e))\n",
    "    plot_pos_map(pos_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "for e in range(episodes):\n",
    "    \n",
    "    pos_map = np.zeros((y_size, x_size))\n",
    "    p, v = track.reset()\n",
    "    for k in range(200):\n",
    "        s_y, s_x = p[0], p[1]\n",
    "        s_vy, s_vx = v[0], v[1]\n",
    "        \n",
    "        pos_map[s_y, s_x] += 1  # exploration map\n",
    "        \n",
    "        action = ???\n",
    "        \n",
    "        a = track.action_to_tuple(action)\n",
    "        action_state = track.state_action((p, v), a)\n",
    "\n",
    "        (p, v), reward, done, _ = ???\n",
    "\n",
    "        if done:\n",
    "            print('Done')\n",
    "            break \n",
    "\n",
    "            \n",
    "    print('Sample trajectory on learned policy in episode {}:'.format(e))\n",
    "    pos_map = (pos_map > 0).astype(np.int16)\n",
    "    pos_map +=  track.course  # overlay track course\n",
    "    plot_pos_map(pos_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Extra Challenge: A More Complex Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The course given below poses a substantially harder challenge for Monte-Carlo based algorithms. Why? If you want to try solving it yourself, be aware that it may take much longer until a successful policy is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the course\n",
    "_course_dim = (8, 10)\n",
    "_inner_wall_dim = (2, 6)\n",
    "\n",
    "def build_rect_course(course_dim, inner_wall_dim):\n",
    "    \"\"\"\n",
    "    Build a race track given specifications for the outer cyclic street and inner wall dimensions.\n",
    "    Start and finish line should be placed in the center top. The course dimension specifications\n",
    "    do not consider a bounding wall around the track, which must be inserted additionally.\n",
    "    \n",
    "    Args:\n",
    "        course_dim: 2-tuple, (y-dim, x-dim): The size of the track without outer walls.\n",
    "        inner_wall_dim: 2-tuple (y-dim, x-dim): The size of the inner wall\n",
    "    \n",
    "    \"\"\"\n",
    "    track = []\n",
    "    wall_up_bound = course_dim[0]//2 - inner_wall_dim[0] // 2\n",
    "    wall_bottom_bound = course_dim[0]//2 + inner_wall_dim[0]//2\n",
    "    street_width = course_dim[1]//2 - inner_wall_dim[1]//2\n",
    "    # construct course line by line\n",
    "    for i in range(course_dim[0]):\n",
    "        if i < wall_up_bound:\n",
    "            half_street_len = course_dim[1]//2 - 1\n",
    "            track_row = 'o'*half_street_len + '+W-' + 'o'*(half_street_len-1)\n",
    "        elif  wall_up_bound <= i < wall_bottom_bound:\n",
    "            track_row = 'o'*street_width + 'W'*inner_wall_dim[1] + 'o'*street_width\n",
    "        else:\n",
    "            track_row = 'o'*course_dim[1]\n",
    "        track.append(track_row)\n",
    "    # add boundary\n",
    "    track = ['W'*course_dim[1]] + track + ['W'*course_dim[1]]\n",
    "    track = ['W'+s+'W' for s in track]\n",
    "    return track\n",
    "    \n",
    "course = build_rect_course(_course_dim, _inner_wall_dim)\n",
    "track = RaceTrackEnv(course)\n",
    "for row in course:\n",
    "    print(row)\n",
    "    \n",
    "pos_map =  track.course  # overlay track course\n",
    "plot_pos_map(pos_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
